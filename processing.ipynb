{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data(train_path): \n",
    "    train = pd.read_csv(train_path)\n",
    "    train_x = list(train['Text'])\n",
    "    train_y = train['Category']\n",
    "    label_map = {label: code for label, code in zip(train_y, train_y.astype('category').cat.codes)}\n",
    "    train_y = train['Category'].astype('category').cat.codes\n",
    "\n",
    "    label_map = dict(sorted(label_map.items(), key = lambda item: item[1]))\n",
    "\n",
    "    return train_x, train_y, label_map\n",
    "\n",
    "dir = r'C:\\Users\\deguz\\OneDrive\\PET_PROJECTS\\Multiclass_Classification-News_Articles'\n",
    "train_x, train_y, label_map  = compile_data(dir + '\\BBC News Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample instance\n",
    "train_x[0][0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample label\n",
    "train_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 0, 'entertainment': 1, 'politics': 2, 'sport': 3, 'tech': 4}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classes\n",
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning: Punctuation, Stopwords, Tokenization --> Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worldcom', 'ex', 'boss', 'launches', 'defence']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"PROCESSING =====================================================\"\"\"\n",
    "def clean_text(samples, n_words = 45):\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "    cleaned = []\n",
    "    for sample in samples: \n",
    "        words = nltk.RegexpTokenizer(\"['\\w]+\").tokenize(sample) #Punc + Tokenize\n",
    "        words = [word.lower() for word in words if word.lower() not in stopwords] #Stopwords\n",
    "        cleaned.append(words)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "train_x = clean_text(train_x)\n",
    "train_x[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TRAIN/TEST SPLIT - to be used across all models\"\"\"\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size = 0.2)\n",
    "\n",
    "names = ['train_x', 'test_x', 'train_y', 'test_y', 'label_map']\n",
    "vars = [train_x, test_x, train_y, test_y, label_map]\n",
    "\n",
    "overwrite_train_test = False\n",
    "if overwrite_train_test: \n",
    "    for n, v in zip(names, vars): \n",
    "        with open(f'vars/{n}.pkl', 'wb') as f: \n",
    "            pickle.dump(v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store model results\n",
    "file_name = 'results.csv'\n",
    "if not os.path.exists(file_name):\n",
    "    results = pd.DataFrame(columns = ['model', 'accuracy', 'weighted_f1'])\n",
    "    results.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Word Vectors (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import process_funcs as pf\n",
    "train_x, test_x, train_y, test_y, label_map  = pf.get_train_test()\n",
    "\n",
    "overwrite_vecs = False\n",
    "if overwrite_vecs: \n",
    "    vec_model = Word2Vec(sentences = train_x, vector_size = 100, min_count = 2, workers = 4)\n",
    "    vec_model.save(\"embeddings/custom_embeddings.model\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7cb1022e853cc79376f316cb56a0ade4081b3989831c5c21d525c392fdd8f794"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('tf-gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
